{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UWAXYt_VESAd"
   },
   "source": [
    "## Importing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zaZM5DjQESAh",
    "outputId": "8ed4bc6f-36a9-44b8-944b-081533f27e43"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "import re\n",
    "from string import punctuation\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import itertools\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# set plot style\n",
    "sns.set(style = 'whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Qf2Zp8LESAi"
   },
   "source": [
    "## Loading Data And Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VOCoiX3WESAj"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train_set.csv')\n",
    "test_df = pd.read_csv('test_set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "id": "AK-bqufJESAj",
    "outputId": "4b70f414-a06d-4553-f8a2-3d973d1ff45e"
   },
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "id": "76FdoboRESAk",
    "outputId": "fa6ebfbf-04d5-4a68-fe00-34d5f6d8ee55"
   },
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j1d89m1sESAl",
    "outputId": "517067bf-6955-48ef-8cc0-dd0face7b75d"
   },
   "outputs": [],
   "source": [
    "np.shape(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "idiGFdyuESAl"
   },
   "source": [
    "The data consist of 2 columns 'lang_id' and 'text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qakjygj3ESAl",
    "outputId": "87be112b-b00a-41c8-ae06-f6398e8d0bc1"
   },
   "outputs": [],
   "source": [
    "list(train_df['lang_id'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tIxMaZy3ESAm"
   },
   "source": [
    "We have 11 different languages. Now we're going to check how they are distributed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "id": "2SWzEzTJESAm",
    "outputId": "301dd4fa-efc8-43c4-d39e-de746726004a"
   },
   "outputs": [],
   "source": [
    "ax = train_df['lang_id'].value_counts().plot(kind = 'bar', \n",
    "                                          title = 'Counts of Each Class of Sentiment',\n",
    "                                          xlabel = 'sentiment', \n",
    "                                          ylabel = 'Count')\n",
    "n_obs = [i.get_height() for i in ax.patches]\n",
    "total = sum(n_obs)\n",
    "\n",
    "for i in ax.patches:\n",
    "    # get_x pulls left or right; get_height pushes up or down\n",
    "    ax.text(i.get_x()+0.06, i.get_height()+0.5, \\\n",
    "            str(round((i.get_height()/total)*100, 1))+'%', fontsize=12,\n",
    "                color='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TtoTgx5jESAn"
   },
   "source": [
    "The observations are balanced throughout all the classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hAA7t5vAESAn"
   },
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "98m6Vb9NESAn"
   },
   "outputs": [],
   "source": [
    "def text_cleaner(text):\n",
    "    #let's removing punctuation\n",
    "    res = text.apply(lambda x: ''.join(i for i in x if i not in punctuation))\n",
    "    res = res.str.lower()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "id": "Rn1TLE4DESAn",
    "outputId": "a5ab4f06-eb7a-4758-a193-014eae67b5bf"
   },
   "outputs": [],
   "source": [
    "train_df['clean_text'] = text_cleaner(train_df['text'])\n",
    "test_df['clean_text'] = text_cleaner(test_df['text'])\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "id": "jKj7Er0wESAo",
    "outputId": "7e7b9234-1ce9-4e6b-a498-5e1b8643516b"
   },
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_XCJKhFAESAo"
   },
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AUBciXqUESAo"
   },
   "outputs": [],
   "source": [
    "X=train_df['clean_text']\n",
    "y=train_df['lang_id']\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.1,random_state =42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KDVtLpRqESAo"
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1,4))\n",
    "lr1 =Pipeline([\n",
    "    ('vectorizer', vectorizer),\n",
    "    ('clf', LogisticRegression(C=1000))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4_5HsjUbESAp"
   },
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L28aYS4TESAp",
    "outputId": "cd58bc4e-e84a-4eb5-e27a-67796f4d95eb"
   },
   "outputs": [],
   "source": [
    "lr1.fit(X_train,y_train)\n",
    "y_pred_lr1=lr1.predict(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "twhsvwWfG9CY",
    "outputId": "955f93bc-57c0-439b-d671-f2533c19664b"
   },
   "outputs": [],
   "source": [
    "print('Model: Logistic Regression')\n",
    "print(classification_report(y_test, y_pred_lr1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PVhmrEicHZwR"
   },
   "outputs": [],
   "source": [
    "test_pred= lr1.predict(test_df['clean_text'])\n",
    "my_submission = pd.DataFrame({'index': test_df['index'], 'lang_id': test_pred})\n",
    "# you could use any filename. We choose submission here\n",
    "my_submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n4x-StuXGs3k"
   },
   "outputs": [],
   "source": [
    "vectorizer2 = TfidfVectorizer(ngram_range=(1,2),strip_accents='ascii')\n",
    "lr2 =Pipeline([\n",
    "    ('vectorizer', vectorizer2),\n",
    "    ('clf', LogisticRegression(C=10))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wz5wG_DNESAp",
    "outputId": "f61d4f55-204e-4bdf-9d39-c97dd5e9a434"
   },
   "outputs": [],
   "source": [
    "lr2.fit(X_train,y_train)\n",
    "y_pred_lr2=lr2.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zV1TRan9LeTC",
    "outputId": "d8ab4838-4944-469a-c941-29cecf996df0"
   },
   "outputs": [],
   "source": [
    "print('Model: Logistic Regression')\n",
    "print(classification_report(y_test, y_pred_lr2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uUOPPzgtMPfF"
   },
   "outputs": [],
   "source": [
    "test_pred2= lr2.predict(test_df['clean_text'])\n",
    "my_submission2 = pd.DataFrame({'index': test_df['index'], 'lang_id': test_pred2})\n",
    "# you could use any filename. We choose submission here\n",
    "my_submission2.to_csv('submission2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nJcM-SOhNDjm"
   },
   "outputs": [],
   "source": [
    "vectorizer3 = CountVectorizer(ngram_range=(1,3),strip_accents='ascii')\n",
    "svc1 =Pipeline([\n",
    "    ('vectorizer', vectorizer3),\n",
    "    ('clf', SVC(kernel='linear',C=10))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b3Nn0ROaNOZK"
   },
   "outputs": [],
   "source": [
    "svc1.fit(X_train,y_train)\n",
    "y_pred_svc1=svc1.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AskMGhr_NaSr",
    "outputId": "917fded6-d567-47af-ff6b-d18feb36c9ed"
   },
   "outputs": [],
   "source": [
    "print('Model: Logistic Regression')\n",
    "print(classification_report(y_test, y_pred_svc1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gPDvICctPdTx"
   },
   "outputs": [],
   "source": [
    "test_pred3= svc1.predict(test_df['clean_text'])\n",
    "my_submission3 = pd.DataFrame({'index': test_df['index'], 'lang_id': test_pred3})\n",
    "# you could use any filename. We choose submission here\n",
    "my_submission3.to_csv('submission3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5o7KJmO_R78d"
   },
   "outputs": [],
   "source": [
    "vectorizer4 = CountVectorizer(ngram_range=(1,3),strip_accents='ascii')\n",
    "svc2 =Pipeline([\n",
    "    ('vectorizer', vectorizer3),\n",
    "    ('clf', SVC(kernel='rbf'))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8LiaeyvGSFlO"
   },
   "outputs": [],
   "source": [
    "svc2.fit(X_train,y_train)\n",
    "y_pred_svc2=svc2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9u5mrOb1SVQb",
    "outputId": "d8f8b80d-ced2-4320-b455-0aaaf272baf7"
   },
   "outputs": [],
   "source": [
    "print('Model: Support Vector Machine')\n",
    "print(classification_report(y_test, y_pred_svc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r520YiAJTOCx"
   },
   "outputs": [],
   "source": [
    "test_pred4= svc2.predict(test_df['clean_text'])\n",
    "my_submission4 = pd.DataFrame({'index': test_df['index'], 'lang_id': test_pred})\n",
    "# you could use any filename. We choose submission here\n",
    "my_submission4.to_csv('submission4.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SyTOxKfsVD5K"
   },
   "outputs": [],
   "source": [
    "vectorizer5 = CountVectorizer(ngr,strip_accents='ascii')\n",
    "rf1=Pipeline([\n",
    "    ('vectorizer', vectorizer5),\n",
    "    ('clf',RandomForestClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EATagcbTcfQI",
    "outputId": "ae0c7358-3981-44d0-baa2-d97693eda27d"
   },
   "outputs": [],
   "source": [
    "rf1.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NQ-zPBJrfXZq",
    "outputId": "caca10e3-ce22-49c5-a887-2bda0ba542f1"
   },
   "outputs": [],
   "source": [
    "y_pred_rf1=rf1.predict(X_test)\n",
    "print('Model: Support Vector Machine')\n",
    "print(classification_report(y_test, y_pred_rf1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hpdn4ILIlXOg"
   },
   "source": [
    "### Multinomial "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UNqO3jR3crew"
   },
   "outputs": [],
   "source": [
    "vectorizer7 = TfidfVectorizer(use_idf=True, strip_accents='ascii',max_df=0.95)\n",
    "X_vector = vectorizer7.fit_transform(X_train)\n",
    "X_test_vector=vectorizer7.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u05LNS_sF-Jf"
   },
   "outputs": [],
   "source": [
    "nb1=MultinomialNB()\n",
    "nb1.fit(X_vector.toarray(),y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AS2fGQp1F-dF",
    "outputId": "f6d2c55f-42a9-4892-d9ac-cd77aa82e656"
   },
   "outputs": [],
   "source": [
    "y_pred_nb1=nb1.predict(X_test_vector.todense())\n",
    "print('Model: Support Vector Machine')\n",
    "print(classification_report(y_test, y_pred_nb1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TKzs2KEdF-xH"
   },
   "outputs": [],
   "source": [
    "test_pred7= nb1.predict(vectorizer7.transform(test_df['clean_text']).todense())\n",
    "my_submission7 = pd.DataFrame({'index': test_df['index'], 'lang_id': test_pred7})\n",
    "# you could use any filename. We choose submission here\n",
    "my_submission7.to_csv('submission7.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "language_identification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
